{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d6e07f-a25c-495d-a37f-01a138414990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (1.63.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\nikit\\anaconda3\\envs\\lang-env\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (4.48.2)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\nikit\\anaconda3\\envs\\lang-env\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: pytesseract in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (0.3.13)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\nikit\\anaconda3\\envs\\lang-env\\lib\\site-packages (1.25.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\nikit\\anaconda3\\envs\\lang-env\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nikit\\anaconda3\\envs\\lang-env\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from openai) (2.10.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\nikit\\anaconda3\\envs\\lang-env\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nikit\\anaconda3\\envs\\lang-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nikit\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai faiss-cpu transformers tiktoken pytesseract pymupdf Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934b7df3-5e5b-424c-be00-63b542a80a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import base64\n",
    "from mimetypes import guess_type\n",
    "from PIL import Image\n",
    "import fitz\n",
    "import pytesseract\n",
    "import tiktoken\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "# Set your OpenAI API key\n",
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f51229d7-78f0-43b0-b382-bed65671e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Utility Functions\n",
    "# --------------------------\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract full text from a PDF using PyMuPDF (fitz).\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    return full_text\n",
    "\n",
    "def ocr_image(image_path):\n",
    "    \"\"\"Extract text from an image using pytesseract.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    return pytesseract.image_to_string(image)\n",
    "\n",
    "def image_to_data_url(image_path: str) -> str:\n",
    "    \"\"\"Convert an image file to a base64 data URL.\"\"\"\n",
    "    mime_type, _ = guess_type(image_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = 'application/octet-stream'\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        encoded = base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
    "    return f\"data:{mime_type};base64,{encoded}\"\n",
    "\n",
    "def split_text_into_chunks(text: str, max_tokens: int = 2000, overlap: int = 100) -> list:\n",
    "    \"\"\"Split text into overlapping chunks based on token count using tiktoken.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + max_tokens\n",
    "        chunk = encoding.decode(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap  # overlap for context\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c584826-a17c-4035-8abc-ee9559ab5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Embedding and Retrieval Functions\n",
    "# --------------------------\n",
    "\n",
    "def get_text_embedding(text: str, model=\"text-embedding-3-small\"): #text-embedding-ada-002\n",
    "    \"\"\"Get text embedding from OpenAI.\"\"\"\n",
    "    response = OpenAIEmbeddings(model=model)\n",
    "    return np.array(response)\n",
    "\n",
    "def build_faiss_index(chunks: list) -> (faiss.IndexFlatL2, list):\n",
    "    \"\"\"Compute embeddings for each chunk and build a FAISS index.\"\"\"\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        emb = get_text_embedding(chunk)\n",
    "        embeddings.append(emb)\n",
    "    embedding_dim = len(embeddings[0])\n",
    "    embedding_matrix = np.array(embeddings)\n",
    "    index = faiss.IndexFlatL2(embedding_dim)\n",
    "    index.add(embedding_matrix)\n",
    "    return index, embeddings\n",
    "\n",
    "def get_clip_image_embedding(image_path: str, clip_model, clip_processor) -> np.array:\n",
    "    \"\"\"Compute the CLIP image embedding for the image.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = clip_model.get_image_features(**inputs)\n",
    "    return outputs.detach().numpy().flatten().astype(\"float32\")\n",
    "\n",
    "def retrieve_relevant_chunks(image_embedding: np.array, index: faiss.IndexFlatL2, chunks: list, k: int = 3) -> str:\n",
    "    \"\"\"Retrieve the top k most relevant chunks based on cosine similarity.\"\"\"\n",
    "    # FAISS by default computes L2 distances.\n",
    "    # Normalize vectors to use cosine similarity.\n",
    "    def normalize(v):\n",
    "        return v / np.linalg.norm(v)\n",
    "    query = normalize(image_embedding).reshape(1, -1)\n",
    "    # Normalize index vectors manually: if your embeddings are not normalized, do it here.\n",
    "    distances, indices = index.search(query, k)\n",
    "    retrieved = [chunks[i] for i in indices[0]]\n",
    "    return \"\\n\\n\".join(retrieved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e308cab2-507f-4a53-9256-d1fed71b6852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Final Mapping Function\n",
    "# --------------------------\n",
    "    \n",
    "def map_image_to_pci_requirement(retrieved_context: str, image_path: str, image_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends a prompt with the extracted PCI-DSS template text and an image\n",
    "    (in data URL format) to GPT-4 Vision and returns the model's response.\n",
    "    \"\"\"\n",
    "    # Extracting text from image:\n",
    "    image_text = ocr_image(image_path)\n",
    "    \n",
    "    # Preprocess the client screenshot image\n",
    "    print(\"Converting image to data URL...\")\n",
    "    image_data_url = image_to_data_url(image_path)\n",
    "    \n",
    "    # Detailed prompt using the PCI-DSS controls text\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in PCI-DSS compliance. \n",
    "    Below is the relevant context extracted from the PCI-DSS Report on Compliance Template containing the controls and requirements.\n",
    "    A client has provided a screenshot showing details of their network and security configuration.\n",
    "    Analyze the image and identify which specific control requirement is being addressed.\n",
    "    Provide the control requirement code along with a detailed explanation of \n",
    "    how the information in the given image satisfies that requirement.\n",
    "    PCI-DSS Template Excerpt: \\n\n",
    "    {retrieved_context}...\\n\n",
    "\n",
    "    Context text from image: \\n\n",
    "    {image_text}\n",
    "    Please be as specific as possible in your mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the messages for the ChatCompletion API.\n",
    "    # The user message is given as an array with both a text segment and the image.\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a PCI-DSS compliance expert.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_data_url}}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Call the GPT-4 Vision API (model name may vary, e.g., \"gpt-4-vision-preview\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # adjust to your available model identifier\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    # Extract and return the answer text from the response.\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e6543aa-1052-4244-b1a6-a241be9997ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Main Application Flow\n",
    "# --------------------------\n",
    "\n",
    "def main():\n",
    "    # Paths to files\n",
    "    pdf_path = \"PCI-DSS-ROC-Template.pdf\"       # your PCI-DSS template PDF\n",
    "    image_path = \"Connfido Network Diagram.png\"  # client's screenshot image\n",
    "\n",
    "    print(\"Extracting PDF text...\")\n",
    "    full_pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    if not full_pdf_text.strip():\n",
    "        print(\"No text extracted from PDF. Check the file.\")\n",
    "        return\n",
    "\n",
    "    # Split PDF text into chunks\n",
    "    chunks = split_text_into_chunks(full_pdf_text, max_tokens=2000, overlap=100)\n",
    "    print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "    # Build FAISS index for text chunks\n",
    "    print(\"Building FAISS index for text chunks...\")\n",
    "    faiss_index, _ = build_faiss_index(chunks)\n",
    "\n",
    "    # Initialize CLIP model and processor for image embeddings\n",
    "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Compute image embedding using CLIP\n",
    "    print(\"Computing image embedding using CLIP...\")\n",
    "    img_emb = get_clip_image_embedding(image_path, clip_model, clip_processor)\n",
    "\n",
    "    # Retrieve relevant text chunks using image embedding for retrieval\n",
    "    print(\"Retrieving relevant text chunks based on image content...\")\n",
    "    retrieved_context = retrieve_relevant_chunks(img_emb, faiss_index, chunks, k=3)\n",
    "    print(\"Retrieved context:\")\n",
    "    print(retrieved_context)\n",
    "\n",
    "    # Extract OCR text from image (optional extra context)\n",
    "    image_text = ocr_image(image_path)\n",
    "\n",
    "    # Map the image to PCI-DSS requirement using GPT-4 Vision\n",
    "    print(\"Mapping image to PCI-DSS requirement...\")\n",
    "    final_response = map_image_to_pci_requirement(retrieved_context, image_path, image_text)\n",
    "    print(\"Final GPT Response:\")\n",
    "    print(final_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "142b5cb2-7fd7-4614-863b-abf74bdde356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting PDF text...\n",
      "Total chunks: 89\n",
      "Building FAISS index for text chunks...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 22\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Build FAISS index for text chunks\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding FAISS index for text chunks...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m faiss_index, _ \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_faiss_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Initialize CLIP model and processor for image embeddings\u001b[39;00m\n\u001b[0;32m     25\u001b[0m clip_model \u001b[38;5;241m=\u001b[39m CLIPModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[43], line 16\u001b[0m, in \u001b[0;36mbuild_faiss_index\u001b[1;34m(chunks)\u001b[0m\n\u001b[0;32m     14\u001b[0m     emb \u001b[38;5;241m=\u001b[39m get_text_embedding(chunk)\n\u001b[0;32m     15\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(emb)\n\u001b[1;32m---> 16\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m embedding_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(embeddings)\n\u001b[0;32m     18\u001b[0m index \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mIndexFlatL2(embedding_dim)\n",
      "\u001b[1;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a87d8c5-b55c-4df7-a188-175b8a6c28da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lang-env]",
   "language": "python",
   "name": "conda-env-lang-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
